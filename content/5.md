# Asynchronous x-road provider/consumer
## Overview
It is usually undesirable that the dynamic complexity of Bob leaks over the system boundary to Alice or vice versa. To prevent this from happening, two buffers are used to separate Alice from the rest of the world. There is a request adapter that, upon receiving an incoming request, validates it for syntactic error and, upon success, posts a message to the requests queue. If that succeeds, a "message understood, received and persisted" response to Bob can be sent immediately. On the other end of the queue, there is a worker that consumes the messages and actually provides the necessary services using the information system. The response to the request is then posted to the response queue from where it is picked up by the response worker who sends it back to Bob via another xRoad service. All active components, the request adapter, response worker and the worker, can be multiplied to provide better scalability. The pattern is depicted on [figure](figure5)

<a name="figure5"></a>![Asynchronous xRoad provider/consumer pattern](gfx/5_comp.png)

## Summary
 * **Paradigm orientation** The pattern is suitable for documents, data and service
 * **Volume preference** Although message queueing and transformation adds overhead, the pattern is still usable for large request volumes
 * **Direction** The pattern is intended for consuming services but, when reversed, can be used for provision equally well
 * **Availability** The pattern is suitable for situations where the availability requirements of the Alice side are much higher than on Bob's side

## Details
The function of components depicted on [previous figure](figure5) is as follows.
The worker reads messages from the queue and forwards them to the information system for processing. The latter process might either be asynchronous or synchronous. In either case, the worker composes a response message and posts it to the response queue. The worker serves an important function as the guardian of the information system by not submitting more messages than is agreed upon. In case multiple worker instances are present, two synchronisation mechanisms must be present: one to prevent maximum throughput limits being breached and another one (should message order be important) assuring messages are forwarded in order. It is important to note that the one to one relationship between request and response messages is not necessary: a single request might generate multiple responses and the other way around. The worker also acts as a translator (todo: insert pattern reference here) by transforming the message format and semantics of the external interface to a format that is suitable for the information system. 

The adapter compiles the request message and implements the service exposed to the third party. It is not subject to throttling and processes requests at a rate they come in. The adapter might be implemented in three fundamental ways. Firstly, it might respond immediately with a "message received, understood and persisted" message. In this case, any further response from the information system is either meaningless (e.g. the request is for data upload) or delivered by some other means. The first implementation option makes the adapter  stateless and thus easily scalable. Secondly, the adapter might wait for the response message to arrive before responding and hide the asynchronous nature of the implementation entirely. In the latter case, the adapter becomes stateful and thus harder to scale. Also, it must implement the logic for providing error messages in case of delayed responses, deal with multiple request issues. Finally, the adapter might take responsibility for delivering the response messages to Bob. [Figure](figure5c) depicts all three implementation options as a sequence diagram.

Regardless of other details, the adapter and worker must implement an identical set of message formats for request response. For the message formats, it is critical that there is a way to correlate request and response messages with each other and with the incoming request from Bob. One of the simplest ways to achieve this is supplying messages with unique identifiers and maintaining a metadata section with the transaction log referencing these. This being is the only dependency between the worker and the adapter highlights the key advantage of this pattern: separation of business logic from the interface logic. Adapter takes care of the latter while worker encapsulates the former.

<a name="figure5c"></a>![Sequence diagram of the asynchronous service provider](gfx/5_seq.png)

As depicted on [figure](figure5c), processing of a request is comprised of following steps:

 1. A request for service is received from the security server
 1. Adapter validates the message against an agreed-upon syntax. At this point, the amount of processing is minimised and only formal validation is conducted. For example, the adapter might verify that an address field is filled but does not confirm if the address in the field is indeed valid.
 1. Adapter transforms the request into a format that can be understood by the worker adding the necessary metadata and, if necessary, generating message identifiers
 1. The message is added to the request queue
 1. In case the response does not carry meaning or will be delivered later, a message is sent to the security server confirming message delivery
 1. The worker reads a message from the request queue 
 1. The message is transformed to a format the information system understands by striping (but not discarding!) headers. It is likely that at this point a protocol transformation also takes place by, for example, making a request towards a CORBA server. 
 1. In case the response from the information system is meaningful, it is transformed into a message the adapter can understand and posted to the response queue
 1.  The adapter server reads a message from the response queue and matches it to a request so it can be delivered
 1.  In case the customer is still waiting, the response is delivered
 1.  If the customer has left, the adapter server takes active measures to deliver the message. In this case, the original message from the security server must contain some call-back mechanism so the adapter knows where a particular message is to be delivered 
 
## SWOT
### Strengths
 * The pattern effectively prevents dynamic complexity leakage across organisational boundaries. While both the worker and the external customer agree to adhere to a certain behaviour, only the worker can be guaranteed to do so. In case the requests come in faster than the information system can handle, the request queue starts growing. Once the situation is reversed, the queue shrinks again. This independence allows for simpler service level agreements with the partner. Also, there is now flexibility around what that behaviour is at a given moment permitting to compliance with service windows and the like. 
 * The timeouts are under control. In any integration scenario, proper handling of timeouts in an explicit manner is of paramount importance as a raw timeout is an uncertain event. There is no knowledge about what exactly happened and on what level the timeout actually occurred. Did the message get delivered and processed with the network timing out during response delivery or was the request not received because of network issues? In a direct integration scenario, the resources of the processing system are busy processing incoming requests and its ability to produce an error log or a meaningful error message is limited. This also means transactional integrity is much better preserved as the state of the exchange is know with much higher probability to both parties 
 * Bob can be released rapidly. In case the response is not meaningful or the reconciliator pattern is used (see [section](section:9)) Bob can rapidly be confirmed that the message has been correctly received releasing its resources. Otherwise Bob might be waiting until the message is fully processed, ready to receive a negative response and act upon it. Such waiting can very resource-intensive for high-throughput use cases. 
 * Transactional integrity is rather well preseverd. If the security server communicated directly with the information system

## Weaknesses
 * The pattern is developmentally complex. Compared to the standard model, there are more moving parts and protocols to agree upon, specify, build and change. Therefore, its upsides in terms of more sensible behaviour towards Bob and better resilience towards his possibly erratic manners must be carefully considered.
 * The pattern is operationally complex containing many more elements to be deployed and maintained. See [section](section5:ops) for a more detailed discussion of this
 *  Business process complexity for handling issues. This is actually a strength in disguise as in case of the standard model, there are many situations where a failed interaction leaves no meaningful traces. The asynchronous pattern however surfaces such issues at either at the worker  (what if Bob was told everything was fine but the information system fails to process the request?) or at the adapter (I have a response message but Bob has dropped the connection. What do I do with the response?). Figuring out what the correct behaviour is might not be trivial but typically the issues are too glaring for developers to simply ignore.

### Opportunities
 * Dynamic scalability. When the standard approach is used, there is no way to react to a rapid increase of load. In case of this pattern, however, the increase of the request queue can be detected. Should that be the case, resources might be added either to information system, the worker or both, so increased load can be handled without architectural changes. Alternatively, the adapter server might gracefully reject some requests
 * Multi-step processing. Since the interaction with the partner is separated from actually processing the message, it is possible to make the processing consist of multiple steps. For example, one might add a step to enrich the incoming message with additional data. TODO: insert pattern reference. Also, the same request can be utilised for multiple purposes. For example, it can be used for both the formal processing by the information system as well as producing statistics. 

### Threats
The main threat in case of this pattern is invalid communication of response code. That threat is, of course, always present but here the palette of possible conditions is much wider making for a more complex semantic space. In case of the standard model, a "OK" response has a definitive meaning of the message being received and processed. Here, "OK" might mean that the message has been received, validated and persisted but subsequently lost due to a failure by the queue provider. Also the boundary between structural and process correctness of messages can be hazy (does the validity of an address mean it is a valid string, that it is a validly formatted address or that the address actually exists in a database somewhere). 

To counter this threat, the semantics of response code as well as the entire behaviour of the interface must be very well documented and the test cases in use ought to test for the semantic boundaries described.

## Operational considerations
<a name="section5:ops"></a>
As mentioned before, one of the weaknesses of the asynchronous service provider pattern is its operational complexity. In addition to having more moving parts, it slightly alters the notion of performance monitoring. Since the components are not rigidly dependent on each other, they can commonly work at their optimal performance point. Thus it might be difficult to determine, if system performance is adequate in relation to incoming load. Commonly, server resources like CPU, memory and threads are monitored for this end, but this pattern is designed to keep these parameters stable. Instead, queue status should be monitored: spikes in incoming (or, outgoing) queue lengths indicate whether or not demand is handled adequately. In general, a more complex system can behave in more complex ways and thus properly thought out monitoring is important for this pattern.

In case the demand can not indeed be satisfied, the adapter should be able to gracefully reject incoming requests. This can be done in multiple ways. Assuming the client is adequately capable, one might respond with HTTP 503 response code and utilise the `Retry-After` header as described in RFC2616. An other, possibly more transparent, option is to modify the API provided to contain this sort of rejection.

Another operational consideration for this pattern is the availability of the queues. Since they play a key role in system availability, they should have a very robust availability model preventing data loss. For these purposes, it is probably a good idea to share the queue provider between different systems so headroom can be pooled.

Finally, this patterns can grow operationally more complex if more than one instance of the Adapter component are present. With one adapter, it can be guaranteed that the same component that maintains a connection to the customer via the SecurityServer also receives a response message. With multiple adapters this is no longer the case. For multiple Adapters to work, the messages passed must contain a reference to the adapter as well as the originating connection and the queue implementation must support ''peeking'' i.e. looking at a message and returning it to the queue untouched.
 
