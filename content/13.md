---
layout: page
title: Read-through cache
permalink: /13/
---

## Overview
Sometimes Alice needs rapid and reliable response from Bob but can live with the information received to be out of date. In that case, Alice can implement a stub service that responds to all queries based on local data or, if that is missing, executes a query towards Bob and caches the result. It might be that both options are used: while a local copy is served for speed, the cache is updated by requesting fresh information from Bob. 

The cache can be implemented both transparently towards the provider service as well as in tight cooperation. In the latter case the provider service can take active control of the cache state by supplying TTL values or even interacting with the cache to invalidate expired entries. 

The pattern is depicted on [figure](#figure13).
<a name="figure13"></a>![Read-through cache pattern](/gfx/13_comp.png)

## Summary
 * **Paradigm orientation** The pattern is suitable for data, can be applied to documents but is not usable for services
 * **Volume preference** The pattern can be used to reduce traffic volumes and thus becomes useful if the complexity added is outweighed by the reduced volume
 * **Direction** The pattern is agnostic towards service direction and could be used by both parties simultaneously
 * **Availability** The pattern is suitable for situations where the availability requirements of the Alice side are much higher than on Bob's side


## Details
The cache pattern has two main components.

The Stub acts as a facade (TODO: GoF reference) towards the rest of the system hiding all interaction logic from the rest of the information system. It exposes the same interface, as the underlying service, fetching the response from a cache, if it exists. The stub can be implemented both generically (i.e. accepting any requests and utilising the same cache behaviour) and specifically (i.e. being tailored to a specific service with custom cache behaviour). 

The cache is responsible for actually storing the objects. Many standard implementations exists (For example memcached available at http://memcached.org) and, as the problem is a very generic one, it is seldom sensible to implement one's own solution. 

An important aspect of the pattern is its behaviour in case of cache misses while Bob is unavailable. Under normal circumstances, a cache miss results in fetching a result directly from Bob. When Bob is not available, however, passing on the semantics of the interaction can become significant. The stub should alert the consumer information system that a response can not be returned by implementing a supplement to the original service interface. As this violates the idea of interface transparency, the supplement should use a distinct set of communication mechanisms to minimise chances of a clash. For example, if an original service interface communicates via return codes, it is difficult to assure that any new return codes invented by the Stub do not clash with the ones returned by Bob and communicating faults via headers is much more desirable.

In case Bob is cache-aware, it is recommended to follow the ideas of cache control mechanisms in HTTP (see sec. 13 of RFC2616). In general, one should always assume the cache pattern to be used on the consumer side and provide adequate means for cache control on the API level. 

An important parameter of the pattern is cache duration, he period of time for which a cached copy is retained. A relatively short period compared to the expected change cadence of the data is recommended as various business logic, security and operational issues increase exponentially relative to the cache duration. 

If a question of persistent or in-memory cache arises, the applicability of the pattern should be questioned. A persistent cache assumes long-lived data that is used frequently but is expensive to query. For these cases, an explicit local replica created using the pub-sub pattern (see section \ref{sec:p:14}) is probably a better idea.

## SWOT

### Strengths
 * Resilience. The pattern allows complete independence between Alice and Bob on temporal basis. The information system of Alice can rely on some sort of response always being available  

### Weaknesses	
 * Should the information exchanged be temporally sensitive (e.g. authorisation information), the business logic must account for the fact that the cache might be out of date creating a dependency between business and technical architectures. This might be achieved by explicitly stating propagation times for information, for example or establishing operational procedures for keeping cache TTL values sufficiently short 
 * The pattern assumes the underlying service not to change its internal state as a result of a service call. Should that not be the case, undefined behaviour arises as the service calls from the client have no relationship to the service calls actually executed on the server
 * As a copy of Bobs data is stored by Alice, the same risk management measures need to be implemented by both parties. If the overall risk level at Alice is higher than at Bob, this does not necessarily present a problem. Should Bob be significantly more risk-averse than Alice, the required increase in risk management measures by Alice might render the pattern unusable

### Opportunities
 * Replica. Should the cache implement a persistent data store, that can be used as local copy of the remote database. Also, the pattern can be in conjunction with the replica using the pub-sub pattern (see [section](/14))
 *  Transparent integration. Since the pattern contains a ``Stub`` that, transparently, provides the service to the information system, the entire interaction logic can be replaced if need be. For example, the solution might be moved from pull to push model and cache replaced with a full replica of the database

### Threats	
 * If the relationship between business logic is not implemented properly, the pattern can potentially expose systems to timing attacks as a window is created during which asynchronicity exists between the two systems

## Operational considerations
The main operational consideration of the cache is the question of centralised or distributed cache. In case of a central cache, all sub-systems of Alice access Bob via the same cache. This, however, creates a single point of failure and a scaling bottleneck while ensuring consistent results for the entire system. Also, an additional network hop is added between nodes and the cache. Local caches for sub-systems or component instances lead to lower latency and a more robust setup but can lead to inconsistencies in system behaviour depending on whether a particular node hits cache or not. 

Another important consideration is the one of cache persistency. Non-persistent cache is operationally simple and offers low latency. Should the memory be lost, however, stub response times spike as all cache requests become misses. Such sudden change in response times can trigger dynamically complex behaviour upstream. A persistent cache, while free from the need to re-population after downtime, is operationally more complex requiring storage and offers higher latency due to IO access involved. With persistent cache, the weakness of risk management is significantly more pronounced.
