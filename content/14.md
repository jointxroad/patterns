---
layout: page
title: Pub-sub 
permalink: /14/
---

## Overview
Let us assume Alice needs to let a potentially large but unspecified number of parties know about events occurring in their information system. A common example of this would be a need to replicate a dataset to a number of locations. In case polling would lead to a overwhelming percentage of requests yield the same result (because of large number or partners or infrequent changes) or availability requirements for Alice are low, a pub-sub pattern would be helpful. 

This pattern means any partner can subscribe and unsubscribe to events and that Alice takes the responsibility to notify subscribers of events as they occur. Alice needs to implement a "subscribe" service while all the partners need to implement a "receive a message" service. Since it would be beneficial isolate the act of sending notifications from any actual events in the information system, an asynchronous mechanism is implemented.

The pattern is depicted on [figure](#figure14).

<a name="figure14"></a>![The pub-sub pattern](/gfx/14_comp.png)

## Summary
 * **Paradigm orientation** The pattern is mainly suitable for data-oriented applications as one of its key use cases is to transmit information of changes in state
 * **Volume preference** The pattern has no direct volume preferences and can be made to work with a very large number of changes
 * **Direction** The pattern can be used on both sides of the service
 * **Availability** The pattern is suitable and useful when one side, the data provider, has smaller availability requirements as the data consumer

## Details
The patterns is relatively complex in nature and a brief overview of functions of the depicted components is as follows.


 * **Pusher** is responsible for extracting event information from the source information system. The event in question might be generated by a trigger in a database, by a business logic workflow or a batch process and the pusher is responsible for preparing the relevant message and pushing it into the queue
 * **Queue** the exact details of the queue implementation are not relevant for the pattern, the only needed functionality is the ability to persist messages, deliver them and provide monitoring facilities. It is however important to keep in mind that the delivery semantics of the queue and the "deliver event" interface match. For example, the latter should not make promises on message ordering if the queue does not
 * **Worker** The worker is tasked with reading messages from the queue and delivering them to the subscribers based on subscription information. For higher loads, it is recommended the actual act of delivering a message to the subscriber is separated from the reading messages from the queue. This way, multiple subscribers can be notified in parallel and a parallel processing framework like Parallel Python can be used to spread the load among multiple nodes. It is also possible to combine the worker component with the asynchronous x-road pattern (see section \ref{sec:p:5}) with separate delivery queues for each consumer. This way one slow receiver will not block other subscribers from receiving their events. It is crucial the failure mode of the worker is documented and clearly communicated as mismatching interpretations of it might lead to loss of information for subscribers
 * **Receiver** is the component on the message receiver side that exposes the callback interface and passes the event on for processing. Again, it is a prime candidate for the asynchronous x-road pattern. 
 * **SubscriptionDB** implements persistence of subscriptions. Since it is not necessarily heavily used and might even be manually managed, as the subscriber component is optional, the implementation might even be a flat file
 * **Subscriber** provides the x-road service for subscribing and unsubscribing. Depending on the implementation, subscription information might include callback information, event types subscribed to or failure modes	

Should the pattern be used for data replication, it would be sensible to combine it with the reconciliator pattern (see section \ref{sec:p:9}) to make sure no events have been missed.

A crucial part of the pattern is the "Deliver event" interface. Its semantics should be well understood and clearly communicated. Because detecting duplicate messages on the producer side is unlikely to be feasible, the messages are most likely going to be delivered *at least* once not *exactly* once. This puts the burden of duplicate checking on the subscriber side. Secondly, the interface could either be defined as message specific (e.g. NotifyOfChangeInAlicesDatabase) or message agnostic (e.g. NotifyOfGenericChange). For protocol transparency, the specific approach is preferred although multiple message types with several partners could make the agnostic design preferable.

## SWOT

### Strengths
 * The pattern allows to implement complex business logic for handling subscriptions while isolating that logic from the source system
 * As the main activity comes from the event source, it allows for integration of systems with a wide range of availability requirements. It might be feasible for Alice to disappear for prolonged periods of time (during which periods polling would be unavailable) and send out bursts of accumulated events

### Weaknesses
 * The pattern is relatively complex with many moving parts. In case asynchronous mechanisms are used for message delivery to subscribers, the system might entail tens of separate components. Therefore the overhead of operational complexity of polling must be very convincing for the pattern to make sense
 * The pattern is relatively failure-prone as the asynchronous delivery mechanisms can make little guarantees about delivery timing or certainty. Therefore, the pattern should be accompanied by the reconciliator pattern assuring mutually compatible understanding
	
### Opportunities
The main opportunity presented by the pattern is to be able to do database replication. Since it could potentially create a master data problem, the opportunity should be explored with caution

### Threats
The key threat to the pattern is a sudden spike of either subscriber numbers or change events. Unless the worker component is implemented with scalability in mind, and proper operational measures are in place to detect such a situation, event delivery could act up in unpredictable ways. Also, the pattern is prone to rapid increase in operational complexity should complex retry mechanisms be put in place for subscribers.

### Operational considerations
The key operational consideration for the pattern is queue and worker state monitoring in accordance with the failure mode(s) implemented. For example, in case the worker will simply drop undeliverable messages do subscribers, this should be clearly logged. And when the worker will use a queue-based approach for pushing out messages, the size of that queue should be monitored. 
